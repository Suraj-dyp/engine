{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "from spacy.en import English\n",
    "\n",
    "parser = English()\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# you can access known words from the parser's vocabulary\n",
    "# nasa = parser.vocab['NASA']\n",
    "\n",
    "# cosine similarity\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower()})\n",
    "\n",
    "# sort by similarity to NASA\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "#\n",
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 300\n",
    "input_embedding_size = 200 # character length\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2\n",
    "#\n",
    "batch_size = 10\n",
    "#encoder_inputs = tf.Variable(tf.zeros(shape=[8, batch_size], dtype=tf.int32), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs = tf.placeholder(shape=[None,None], dtype=tf.float32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=None, dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "decoder_targets = tf.placeholder(shape=[None, None], dtype=tf.float32, name='decoder_targets')\n",
    "#\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "# embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0),\n",
    "#                         dtype=tf.float32, name='embeddings')\n",
    "\n",
    "# encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "#\n",
    "\n",
    "## Encoder\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\n",
    "#\n",
    "\n",
    "with tf.variable_scope('forward'):\n",
    "    encoder_cell_fw = LSTMCell(encoder_hidden_units)\n",
    "\n",
    "with tf.variable_scope('backward'):\n",
    "    encoder_cell_bw = LSTMCell(encoder_hidden_units)\n",
    "#\n",
    "\n",
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    "(encoder_fw_final_state,\n",
    " encoder_bw_final_state)) = (\n",
    "     tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell_fw,\n",
    "                                     cell_bw=encoder_cell_bw,\n",
    "                                     inputs=encoder_inputs,\n",
    "                                     sequence_length=encoder_inputs_length,\n",
    "                                     dtype=tf.float32,\n",
    "                                     time_major=True)\n",
    " )\n",
    "\n",
    "#\n",
    "\n",
    "## concatenate the tensors along one dimension.\n",
    "\n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs),\n",
    "                            2, name='concat_encoder_outputs')\n",
    "\n",
    "encoder_final_state_c = tf.concat((encoder_fw_final_state.c, encoder_bw_final_state.c),\n",
    "                                  1, name='concat_final_state_c')\n",
    "\n",
    "encoder_final_state_h = tf.concat((encoder_fw_final_state.h, encoder_bw_final_state.h),\n",
    "                                  1, name='concat_final_state_h')\n",
    "\n",
    "# tf tuple used by lstm cells for state_size, zero_state and output state\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n",
    "#\n",
    "\n",
    "\n",
    "## Decoder\n",
    "\n",
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "\n",
    "#\n",
    "\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs), name='unstack')\n",
    "\n",
    "decoder_lengths = encoder_inputs_length + 3\n",
    "#\n",
    "\n",
    "## Output Projection\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1),\n",
    "                dtype=tf.float32, name='weight')\n",
    "\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype= tf.float32)\n",
    "#\n",
    "\n",
    "## Decoder via tf.nn.raw_rnn\n",
    "\n",
    "# assert EOS == 1 and PAD == 0\n",
    "\n",
    "# eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "# pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "# # retrive rows from parm tensors.\n",
    "# eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "# pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n",
    "# #\n",
    "\n",
    "\n",
    "## Manually specifying loop function through time\n",
    "\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)   # all false at the initial stage\n",
    "\n",
    "    #end of sentence\n",
    "    initial_input = None\n",
    "\n",
    "    # last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "\n",
    "    initial_cell_ouput = None\n",
    "    initial_loop_state = None\n",
    "\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_ouput,\n",
    "            initial_loop_state)\n",
    "#\n",
    "\n",
    "## Attention mechanism - choose which previously generated token to pass as input in the next time steps\n",
    "\n",
    "def loop_fn_transition(time,\n",
    "                       previous_output,\n",
    "                       previous_state,\n",
    "                       previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        # dot product betn previous output weights, then+ biases\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "\n",
    "        #prediction = tf.argmax(output_logits, axis=1)\n",
    "        #next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return output_logits\n",
    "\n",
    "    elements_finished = (time >= decoder_lengths)\n",
    "    # this operation produces bool tensor of batch_size\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    #inputp = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "\n",
    "    # set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished,\n",
    "            finished,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "decoder_ouputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_ouputs = decoder_ouputs_ta.stack()\n",
    "\n",
    "#\n",
    "\n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_ouputs))\n",
    "\n",
    "decoder_outputs_flat = tf.reshape(decoder_ouputs, (-1, decoder_dim))\n",
    "\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "\n",
    "#\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "\n",
    "## optimizer\n",
    "\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=decoder_targets,\n",
    "    logits=decoder_logits)\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#\n",
    "## Training on the toy task\n",
    "batch_size = 100\n",
    "batches = helpers.random_sequences(batch_size=batch_size, length_from=3,\n",
    "                                   length_to=8, vocab_lower=0, vocab_upper=1000)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:1]:\n",
    "    print(seq)\n",
    "\n",
    "#\n",
    "datalist = []\n",
    "with open('./uk_data.txt') as dataf:\n",
    "    datalist = [(str(w)).strip() for w in dataf]\n",
    "\n",
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    inputs_, encoder_inputs_lengths_ = helpers.batch(batch)\n",
    "    # decoder_targets_, _ = helpers.batch([\n",
    "    #     (sequence) + [EOS] + [PAD] * 2 for sequence in batch\n",
    "    # ])\n",
    "    # print(type(batch[0]))\n",
    "\n",
    "    # inputs_ is the list of all the index that are used for the batches\n",
    "    encoder_inputs_ = np.ndarray([parser.vocab[datalist[i]].vector for i in inputs_])\n",
    "\n",
    "    # decoder_targets_, _ = helpers.batch([\n",
    "    #     ([i*2 for i in sequence]) + [EOS] + [PAD] * 2 for sequence in batch\n",
    "    # ])\n",
    "\n",
    "    \n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_inputs_lengths_,\n",
    "        decoder_targets: encoder_inputs_,\n",
    "    }\n",
    "\n",
    "#\n",
    "\n",
    "loss_track = []\n",
    "\n",
    "max_batches = 30001\n",
    "batches_in_epoch = 500\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('    minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "\n",
    "            for i, (inp, pred) in enumerate(zip(sess.run(encoder_inputs, fd),\n",
    "                                                np.array(predict_))):\n",
    "                allWords.sort(key=lambda w: cosine(w.vector, inp.vector))\n",
    "                allWords.reverse()\n",
    "                print('    sample {}'.format(i + 1))\n",
    "                print('        input        > {}'.format(allWords[0]))\n",
    "                allWords.sort(key=lambda w: cosine(w.vector, pred.vector))\n",
    "                allWords.reverse()\n",
    "                print('        predicted    > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training Interrupted')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
